Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 144, in <module>
    run_lm(args.vocab_size, args.context_length, args.batch_size, args.device, args.steps, config)
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 104, in run_lm
    logits = model(x)
             ^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 247, in forward
    x = layer(x)
        ^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 380, in forward
    x_attn = self.attn(self.ln1(x))
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 516, in forward
    attn_output = scaled_dot_product_attention(K=K, Q=Q, V=V, mask=causal_mask)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 54, in annotated_scaled_dot_product_attention
    attention_scores = einsum(Q, K, "... query d_k, ... key d_k -> ... query key") / math.sqrt(d_k)
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 80.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 75.06 MiB is free. Including non-PyTorch memory, this process has 21.90 GiB memory in use. Of the allocated memory 21.46 GiB is allocated by PyTorch, and 158.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Benchmarking model size: large
  d_model=1280, d_ff=5120, num_layers=36, num_heads=20
============================================================
Generated:
