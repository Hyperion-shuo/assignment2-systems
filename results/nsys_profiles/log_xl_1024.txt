Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 144, in <module>
    run_lm(args.vocab_size, args.context_length, args.batch_size, args.device, args.steps, config)
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 104, in run_lm
    logits = model(x)
             ^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 247, in forward
    x = layer(x)
        ^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 380, in forward
    x_attn = self.attn(self.ln1(x))
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/model.py", line 516, in forward
    attn_output = scaled_dot_product_attention(K=K, Q=Q, V=V, mask=causal_mask)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/.local/share/uv/python/cpython-3.12.10-linux-x86_64-gnu/lib/python3.12/contextlib.py", line 81, in inner
    return func(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336_systems/basic_lm_nsys_profile_run.py", line 60, in annotated_scaled_dot_product_attention
    attention_weights = softmax(attention_scores, dim=-1)  # Softmax over the key dimension
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/svenshen/cs336/assignment2-systems/cs336-basics/cs336_basics/nn_utils.py", line 6, in softmax
    exponentiated_rescaled_input = torch.exp(rescaled_input)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 63.06 MiB is free. Including non-PyTorch memory, this process has 21.91 GiB memory in use. Of the allocated memory 21.55 GiB is allocated by PyTorch, and 86.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

============================================================
Benchmarking model size: xl
  d_model=1600, d_ff=6400, num_layers=48, num_heads=25
============================================================
Generated:
